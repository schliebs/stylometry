{
    "collab_server" : "",
    "contents" : "source(\"R/packages.R\")\n\n# Data loading and preparation\n\n# Read Emails and immidiately remove empty mails\ndata <- read.csv(\"../data/Emails.csv\")\nnames(data)\n\n# delete empty mails\ndata <- data %>% filter(!is.na(ExtractedBodyText) & ExtractedBodyText != \"\")\n\n\n# Read other Metadata\npersons <- read.csv(\"../data/Persons.csv\")\nreceivers <- read.csv(\"../data/EmailReceivers.csv\")\naliases <- read.csv(\"../data/Aliases.csv\")\n\n#Overview over data\nnames(data)\n\n# Pick 3 frequent authors: \n\ndata$MetadataFrom %>% table() %>% sort()\ndata$sender <- data$MetadataFrom %>% as.character() \n#ggplot(data = data) + geom_histogram(aes(x = sender),stat = \"count\")\n\n# Of 290 unique authors, 16 wrote more than 50 E-Mails. \n# These 16 will be kept for different combinations of training datasets\ndata$sender %>% unique() %>% length()\ndata$sender %>% table() %>% sort() %>% .[.>50] \n\n\n# Advanced Data Management: tm-package/Corpus operations\n\ndata <- data[1:1000,]\n\n# E-Mail Content: \ndocs_raw <- (VCorpus(VectorSource(data$ExtractedBodyText)))\ndocs <- docs_raw \n\nsummary(docs)   \n#inspect(docs[1])\n#writeLines(as.character(docs[2]))\n\n\n# Removing Punctuation\ndocs <- tm_map(docs,removePunctuation)   \n\n# Removing numbers\ndocs <- tm_map(docs, removeNumbers)   \n\n# converting to lowercase\ndocs <- tm_map(docs, tolower)   \n\n# Remove stopwords (do I really wanna do this?)\nlength(stopwords(\"english\"))   \nstopwords(\"english\")   \n# docs <- tm_map(docs, removeWords, stopwords(\"english\"))   \n# docs <- tm_map(docs, PlainTextDocument)\n\n# Remove other particular words\ndocs <- tm_map(docs, removeWords, c(\"studienstiftung\", \"merkel\"))   \n\n## Optional: Stemming the end\n#docs_st <- tm_map(docs, stemDocument)   \n#docs_st <- tm_map(docs_st, PlainTextDocument)\n#writeLines(as.character(docs_st[1])) # Check to see if it worked.\n# docs <- docs_st\n\n\ndocs_final <- docs\n# so far so good the processing\n\ndocs <- tm_map(docs_final, PlainTextDocument)\n\n# DTM\ndtm <- DocumentTermMatrix(docs)   \n#dtm   \n#as.matrix(dtm)\n\n#TDM\ntdm <- TermDocumentMatrix(docs)   \n#tdm   \n#as.matrix(tdm)\n\nfreq <- colSums(as.matrix(dtm))   \nlength(freq) \n\n#  Start by removing sparse terms:   \n#dtms <- removeSparseTerms(dtm, 0.2) # This makes a matrix that is 20% empty space, maximum.   \n#dtms\n\n# Word freq\nfreq <- colSums(as.matrix(dtm))\n\nhead(table(freq), 20) # The \", 20\" indicates that we only want the first 20 frequencies. Feel free to change that number.\ntail(table(freq), 20) # The \", 20\" indicates that we only want the last 20 frequencies.  Feel free to change that number, as needed.\n\nfreq <- colSums(as.matrix(dtm))   \nfreq   \n\nwf <- data.frame(word=names(freq), freq=freq)   \nhead(wf)  \n\n# library(ggplot2)   \n# \n# p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +\n#   geom_bar(stat = \"identity\") + \n#   theme(axis.text.x=element_text(angle=45, hjust=1))\n# p   \n\n\n############################################################\n\n\n## M: Total number of words\n\nallWords = function(d) {\n  return((strsplit(d, \" \")[[1]]))\n}\n\ncountWords = function(d) {\n  return(length(strsplit(d, \" \")[[1]]))\n}\n\n\ndata$number_words <- as.vector(sapply(docs,function(x) countWords(as.character(x[[1]]))))\n\n##C = total number of characters in e-mail body.\n\ndata$total_char <- as.vector(sapply(docs,function(x) nchar(as.character(x[[1]]))))\n\n## V = unique words\n\nuniqueWords = function(d) {\n  return(unique(strsplit(d, \" \")[[1]]))\n}\n\n#the length of a vector containing all unique words\ndata$unique_words <- data$total_char <- as.vector(sapply(docs,function(x) length(uniqueWords(as.character(x[[1]])))))\n\n\n##Number of blank lines/total number of lines\n# yet to implement\n\n## Average sentence length\n\nsentenceLength <- function(x){\n  return(sapply(unlist(str_split(x, boundary(\"sentence\"))), function(x) nchar(x)) %>% mean(na.rm = TRUE))\n}\n\ndata$sentence_length <-  as.vector(sapply(docs,function(x) sentenceLength(x)))\n\n##Average word length (number of characters)\n\ndata$average_word_length <- as.vector(sapply(docs,function(x) mean(sapply(allWords(as.character(x[[1]])),function(x) nchar(x)),na.rm = TRUE)))\n\n\n## Vocabulary richness i.e., V=M\n\ndata$vocab_richness <- data$unique_words/data$number_words\n\n##Total number of function words/M\n# yti\ndata$number_stopwords <- as.vector(sapply(docs,function(x) {length(allWords(x[[1]]) %>% .[.%in% stopwords(\"english\")])} ))\n\ndata$rate_stopwords <- data$number_stopwords/data$number_words\n\n##Function word frequency distribution (122 features)\n# yet to implement!!!\n\n##Total number of short words/M \n\n##Count of hapax legomena/M\n# ??? \n\n##Count of hapax legomena/V\n# ???\n\n### Total number of characters in words/C\n# yti\n\n##Total number of alphabetic characters in words/C\n# yti\n\n##Total number of upper-case characters in words/C\n# yti\n\n##Total number of digit characters in words/C\n# yti\n\n##Total number of white-space characters/C\n# yti\n\n##Total number of space characters/C\n# yti\n\n##Total number of space characters/number white-space characters\n# yti\n\n##Total number of tab spaces/C\n# yti\n\n##Total number of tab spaces/number white-space characters\n# yti\n\n##Total number of punctuations/C\n# yti\n\n##Word length frequency distribution/M (30 features)\n# ???\n\n\n\n\n",
    "created" : 1505654949262.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1193953874",
    "id" : "FECD88E5",
    "lastKnownWriteTime" : 1505684249,
    "last_content_update" : 1505684249480,
    "path" : "C:/Users/Schliebs/OneDrive/17_stylometry/stylometry/R/data_prep.R",
    "project_path" : "R/data_prep.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}