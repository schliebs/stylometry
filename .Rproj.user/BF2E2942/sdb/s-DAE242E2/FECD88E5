{
    "collab_server" : "",
    "contents" : "source(\"R/packages.R\")\n\n# Data loading and preparation\n\n# Read Emails and immidiately remove empty mails\ndata <- read.csv(\"../data/Emails.csv\")\nnames(data)\n\n# delete empty mails\ndata <- data %>% filter(!is.na(ExtractedBodyText) & ExtractedBodyText != \"\")\n\n\n# Read other Metadata\npersons <- read.csv(\"../data/Persons.csv\")\nreceivers <- read.csv(\"../data/EmailReceivers.csv\")\naliases <- read.csv(\"../data/Aliases.csv\")\n\n#Overview over data\nnames(data)\n\n# Pick 3 frequent authors: \n\n\n# Advanced Data Management: tm-package/Corpus operations\n\n\n# E-Mail Content: \ndocs_raw <- (VCorpus(VectorSource(data$ExtractedBodyText)))\ndocs <- docs_raw \n\nsummary(docs)   \n#inspect(docs[1])\n#writeLines(as.character(docs[2]))\n\n\n# Removing Punctuation (loss of information)\n#docs <- tm_map(docs,removePunctuation)   \n\n# Removing numbers (Do not want this: loss of information)\n#docs <- tm_map(docs, removeNumbers)   \n\n# converting to lowercase (loss of information)\n#docs <- tm_map(docs, tolower)   \n\n# Remove stopwords (do I really wanna do this?) => nope\nlength(stopwords(\"english\"))   \nstopwords(\"english\")   \n# docs <- tm_map(docs, removeWords, stopwords(\"english\"))   \n# docs <- tm_map(docs, PlainTextDocument)\n\n# Remove other particular words\ndocs <- tm_map(docs, removeWords, c(\"studienstiftung\", \"merkel\"))   \n\n## Optional: Stemming the end\n#docs_st <- tm_map(docs, stemDocument)   \n#docs_st <- tm_map(docs_st, PlainTextDocument)\n#writeLines(as.character(docs_st[1])) # Check to see if it worked.\n# docs <- docs_st\n\n# so far so good the processing\ndocs <- tm_map(docs_final, PlainTextDocument)\n\n\n\n\n\n########## Document Term / Term Document Matrices ##########\n\n# DTM\ndtm <- DocumentTermMatrix(docs)   \n#dtm   \n#as.matrix(dtm)\n\nfreq <- colSums(as.matrix(dtm))   \nfreq   \n\nwf <- data.frame(word=names(freq), freq=freq)   \nwf_ordered <- wf[order(wf$freq,decreasing = T),]\n\n#TDM\ntdm <- TermDocumentMatrix(docs)   \n#tdm   \ntdm_matrix <- as.matrix(tdm)\nsub_matrix <- tdm_matrix[rownames(tdm_matrix) %in% wf_ordered[1:122,]$word,]\ncolnames(sub_matrix) <- 1:ncol(sub_matrix)\n\nfreq <- colSums(as.matrix(dtm))   \nlength(freq) \n\n#  Start by removing sparse terms:   \n#dtms <- removeSparseTerms(dtm, 0.2) # This makes a matrix that is 20% empty space, maximum.   \n#dtms\n#??? \n\n# Word freq\nfreq <- colSums(as.matrix(dtm))\n\nhead(table(freq), 20) # The \", 20\" indicates that we only want the first 20 frequencies. Feel free to change that number.\ntail(table(freq), 20) # The \", 20\" indicates that we only want the last 20 frequencies.  Feel free to change that number, as needed.\n\n\n\n############################################################\n\n\n## M: Total number of words\n\nallWords = function(d) {\n  return((strsplit(d, \" \")[[1]]))\n}\n\ncountWords = function(d) {\n  return(length(strsplit(d, \" \")[[1]]))\n}\n\n\ndata$number_words <- as.vector(sapply(docs,function(x) countWords(as.character(x[[1]]))))\n\n##C = total number of characters in e-mail body.\n\ndata$total_char <- as.vector(sapply(docs,function(x) nchar(as.character(x[[1]]))))\n\n## V = unique words\n\nuniqueWords = function(d) {\n  return(unique(strsplit(d, \" \")[[1]]))\n}\n\n#the length of a vector containing all unique words\ndata$unique_words <- data$total_char <- as.vector(sapply(docs,function(x) length(uniqueWords(as.character(x[[1]])))))\n\n\n##Number of blank lines/total number of lines\n# yet to implement\n\n## Average sentence length\n\nsentenceLength <- function(x){\n  return(sapply(unlist(str_split(x, boundary(\"sentence\"))), function(x) nchar(x)) %>% mean(na.rm = TRUE))\n}\n\ndata$sentence_length <-  as.vector(sapply(docs,function(x) sentenceLength(x)))\n\n##Average word length (number of characters)\n\ndata$average_word_length <- as.vector(sapply(docs,function(x) mean(sapply(allWords(as.character(x[[1]])),function(x) nchar(x)),na.rm = TRUE)))\n\n\n## Vocabulary richness i.e., V=M\n\ndata$vocab_richness <- data$unique_words/data$number_words\n\n\n## STOPWORDS ? \n\ndata$number_stopwords <- as.vector(sapply(docs,function(x) {length(allWords(x[[1]]) %>% .[.%in% stopwords(\"english\")])} ))\ndata$rate_stopwords <- data$number_stopwords/data$number_words\n\n\n##Total number of function words/M\n# yti\n\n\n##Function word frequency distribution (122 features)\ntest = (sub_matrix / t(rep.col(data$number_words,nrow(sub_matrix)))) %>% t()\n\n##Total number of short words/M \n# defined as <4\n\ncountShortWords = function(d) {\n  return(length(strsplit(d, \" \")[[1]] %>% .[str_length(.) < 4]) )\n}\n\ndata$total_shortwords <- as.vector(sapply(docs,function(x) (countShortWords(as.character(x[[1]])))))\ndata$rate_shortwords <- data$total_shortwords/data$number_words\n\n\n##Count of hapax legomena/M\n# does not work because almost all mails to short!!!\n\n##Count of hapax legomena/V\n# does not work because almost all mails to short!!!\n\n### Total number of characters in words/C\n# \n\n##Total number of alphabetic characters in words/C\n# yti\n\n##Total number of upper-case characters in words/C\n# yti\n\n##Total number of digit characters in words/C\n# yti\n\n##Total number of white-space characters/C\n# yti\n\n##Total number of space characters/C\n# yti\n\n##Total number of space characters/number white-space characters\n# yti\n\n##Total number of tab spaces/C\n# yti\n\n##Total number of tab spaces/number white-space characters\n# yti\n\n##Total number of punctuations/C\n# yti\n\n##Word length frequency distribution/M (30 features)\n# ???\n\n### Structural Attributes (all not implementable)\n\n## Has a greeting acknowledgment\n## Uses a farewell acknowledgment\n## Contains signature text\n## Number of attachments\n## Position of requoted text within e-mail body\n## HTML tag frequency distribution/total number of HTML tags (16 features)\n\n\n## Other structural stuff\n\n\n\n\n#######################################################################\n## Save Data\n\nsaveRDS(data,file = \"../data/processed_data.rds\")\n\n\n",
    "created" : 1505654949262.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1645232098",
    "id" : "FECD88E5",
    "lastKnownWriteTime" : 1505737117,
    "last_content_update" : 1505737117583,
    "path" : "C:/Users/Schliebs/OneDrive/17_stylometry/stylometry/R/data_prep.R",
    "project_path" : "R/data_prep.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}